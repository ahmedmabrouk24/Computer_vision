{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "177U27L-TQT7F8eAMyJowwDmHzCkErBIt",
      "authorship_tag": "ABX9TyMHNzt9Qpcc+dVghjksQyXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedmabrouk24/Computer_vision/blob/main/Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tflearn "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmzWo4g4u89I",
        "outputId": "e40ba44d-eb73-4a7a-af7a-7da8efab6550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 36.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 20 kB 40.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30 kB 47.1 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81 kB 24.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 92 kB 25.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 107 kB 27.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=80391160a5a1eddffb48172b8ccd2b79e492e7f19e12f1abd38425e44f492192\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyKJ1f1WSK10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "24042a68-b16f-4b81-9970-3a68124727f1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f7cd7e986cdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgaug\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmenters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mipyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipyplot'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers,models,Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Activation, Flatten, Dropout, BatchNormalization, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from keras.saving.save import load_model\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tflearn\n",
        "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.estimator import regression\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "import ipyplot\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR_A = '/content/drive/MyDrive/Computer Vision/Dataset_1/personA/Train'\n",
        "TRAIN_DIR_B = '/content/drive/MyDrive/Computer Vision/Dataset_1/personB/Train'\n",
        "TRAIN_DIR_C = '/content/drive/MyDrive/Computer Vision/Dataset_1/personC/Train'\n",
        "TRAIN_DIR_D = '/content/drive/MyDrive/Computer Vision/Dataset_1/personD/Train'\n",
        "TRAIN_DIR_E = '/content/drive/MyDrive/Computer Vision/Dataset_1/personE/Train'\n",
        "TRAIN_DIR=[]\n",
        "TRAIN_DIR.append(TRAIN_DIR_A)\n",
        "TRAIN_DIR.append(TRAIN_DIR_B)\n",
        "TRAIN_DIR.append(TRAIN_DIR_C)\n",
        "TRAIN_DIR.append(TRAIN_DIR_D)\n",
        "TRAIN_DIR.append(TRAIN_DIR_E)\n",
        "\n",
        "\n",
        "Test_DIR_A = '/content/drive/MyDrive/Computer Vision/Dataset_1/personA/Test'\n",
        "Test_DIR_B = '/content/drive/MyDrive/Computer Vision/Dataset_1/personB/Test'\n",
        "Test_DIR_C = '/content/drive/MyDrive/Computer Vision/Dataset_1/personC/Test'\n",
        "Test_DIR_D = '/content/drive/MyDrive/Computer Vision/Dataset_1/personD/Test'\n",
        "Test_DIR_E = '/content/drive/MyDrive/Computer Vision/Dataset_1/personE/Test'\n",
        "Test_DIR=[]\n",
        "Test_DIR.append(Test_DIR_A)\n",
        "Test_DIR.append(Test_DIR_B)\n",
        "Test_DIR.append(Test_DIR_C)\n",
        "Test_DIR.append(Test_DIR_D)\n",
        "Test_DIR.append(Test_DIR_E)\n",
        "\n",
        "train_images=[]\n",
        "test_images=[]\n",
        "\n",
        "IMG_SIZE = 200\n",
        "num_classes = 5\n",
        "MODEL_NAME = 'signature_model1'\n"
      ],
      "metadata": {
        "id": "QoqKQhetg5L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_data():\n",
        "    training_data = []\n",
        "\n",
        "    for path_TRAIN_DIR in tqdm(TRAIN_DIR):\n",
        "       for img in os.listdir(path_TRAIN_DIR):\n",
        "        if 'csv' in img:\n",
        "          continue\n",
        "        path = os.path.join(path_TRAIN_DIR, img)\n",
        "        img_data = cv2.imread(path,0) \n",
        "        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
        "        training_data.append([np.array(img_data),ord(img[6])-ord('A') ])\n",
        "\n",
        "    random.shuffle(training_data)\n",
        "\n",
        "    return training_data\n"
      ],
      "metadata": {
        "id": "AivLBvlWj5QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_test_data():\n",
        "    test_data = []\n",
        "\n",
        "    for path_Test_DIR in tqdm(Test_DIR):\n",
        "       for img in os.listdir(path_Test_DIR):\n",
        "        if 'csv' in img:\n",
        "          continue\n",
        "        path = os.path.join(path_Test_DIR, img)\n",
        "        img_data = cv2.imread(path,0) \n",
        "        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
        "        test_data.append([np.array(img_data),ord(img[6])-ord('A') ])\n",
        "       \n",
        "\n",
        "    random.shuffle(test_data)\n",
        "    return test_data\n"
      ],
      "metadata": {
        "id": "j-ujRT0IsCax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = create_train_data()\n",
        "X_train = np.array([i[0] for i in train_images]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_train = np.array([i[1] for i in train_images]).reshape(-1,)\n",
        "\n",
        "test_images = create_train_data()\n",
        "X_test = np.array([i[0] for i in test_images]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_test = np.array([i[1] for i in test_images]).reshape(-1,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eejyrnehsvU_",
        "outputId": "03d111a9-0619-4218-8693-4b7ee692e98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:52<00:00, 10.43s/it]\n",
            "100%|██████████| 5/5 [00:00<00:00,  5.56it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model1 = Sequential([\n",
        "        layers.Conv2D(filters = 32 ,kernel_size = (3, 3), input_shape = (IMG_SIZE, IMG_SIZE, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size = (2,2)),\n",
        "\n",
        "        layers.Conv2D(filters = 64 ,kernel_size = (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    \n",
        "        layers.Conv2D(filters = 64 ,kernel_size = (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    \n",
        "        layers.Conv2D(filters = 32 ,kernel_size = (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "  \n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1000, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        layers.Dense(5, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model1.compile(\n",
        "    optimizer= tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy'])\n",
        "    \n",
        "model1.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 5)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS-3mhEZt_mQ",
        "outputId": "51897f01-938f-4076-f115-1d224b2ec3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 200 samples, validate on 200 samples\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - ETA: 0s - loss: 3.2704 - acc: 0.3600"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/engine/training_v1.py:2045: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r200/200 [==============================] - 22s 108ms/sample - loss: 3.2704 - acc: 0.3600 - val_loss: 1.6810 - val_acc: 0.2000\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 3s 17ms/sample - loss: 1.6885 - acc: 0.5750 - val_loss: 2.2442 - val_acc: 0.2500\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 3s 17ms/sample - loss: 0.8220 - acc: 0.7500 - val_loss: 2.4874 - val_acc: 0.4350\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 3s 17ms/sample - loss: 0.6213 - acc: 0.8000 - val_loss: 2.3007 - val_acc: 0.4350\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 3s 17ms/sample - loss: 0.3599 - acc: 0.8850 - val_loss: 2.5028 - val_acc: 0.3650\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd7f4ff94f0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}