{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "177U27L-TQT7F8eAMyJowwDmHzCkErBIt",
      "authorship_tag": "ABX9TyN/f6N7Q91ogsjsbmCWHOwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedmabrouk24/Computer_vision/blob/main/person_Recognetion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tflearn "
      ],
      "metadata": {
        "id": "BmzWo4g4u89I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9fc7e1a-f659-4df1-bdfd-bf0d27d6aa90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=8f4cf11f0e8326d812c349ea993cad96e3d8561e239a2c35643eec18b5c67bc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfMPjgM2oZH_",
        "outputId": "2d8ef0cb-690b-43df-abd9-7cb691204de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.8/dist-packages (from h5py) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyKJ1f1WSK10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157b3bc1-5a5a-40e1-91dd-b8ed79af363b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras import layers,models,Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Activation, Flatten, Dropout, BatchNormalization, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from keras.saving.save import load_model\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tflearn\n",
        "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.estimator import regression\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DIR_A = '/content/drive/MyDrive/Computer Vision/Dataset_1/personA/Train'\n",
        "TRAIN_DIR_B = '/content/drive/MyDrive/Computer Vision/Dataset_1/personB/Train'\n",
        "TRAIN_DIR_C = '/content/drive/MyDrive/Computer Vision/Dataset_1/personC/Train'\n",
        "TRAIN_DIR_D = '/content/drive/MyDrive/Computer Vision/Dataset_1/personD/Train'\n",
        "TRAIN_DIR_E = '/content/drive/MyDrive/Computer Vision/Dataset_1/personE/Train'\n",
        "TRAIN_DIR=[]\n",
        "TRAIN_DIR.append(TRAIN_DIR_A)\n",
        "TRAIN_DIR.append(TRAIN_DIR_B)\n",
        "TRAIN_DIR.append(TRAIN_DIR_C)\n",
        "TRAIN_DIR.append(TRAIN_DIR_D)\n",
        "TRAIN_DIR.append(TRAIN_DIR_E)\n",
        "\n",
        "\n",
        "Test_DIR_A = '/content/drive/MyDrive/Computer Vision/Dataset_1/personA/Test'\n",
        "Test_DIR_B = '/content/drive/MyDrive/Computer Vision/Dataset_1/personB/Test'\n",
        "Test_DIR_C = '/content/drive/MyDrive/Computer Vision/Dataset_1/personC/Test'\n",
        "Test_DIR_D = '/content/drive/MyDrive/Computer Vision/Dataset_1/personD/Test'\n",
        "Test_DIR_E = '/content/drive/MyDrive/Computer Vision/Dataset_1/personE/Test'\n",
        "Test_DIR=[]\n",
        "Test_DIR.append(Test_DIR_A)\n",
        "Test_DIR.append(Test_DIR_B)\n",
        "Test_DIR.append(Test_DIR_C)\n",
        "Test_DIR.append(Test_DIR_D)\n",
        "Test_DIR.append(Test_DIR_E)\n",
        "\n",
        "\n",
        "train_images=[]\n",
        "test_images=[]\n",
        "\n",
        "\n",
        "IMG_SIZE = 200\n",
        "num_classes = 5\n",
        "MODEL_NAME = 'signature_model1'\n"
      ],
      "metadata": {
        "id": "QoqKQhetg5L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_train_data():\n",
        "    training_data = []\n",
        "\n",
        "    for path_TRAIN_DIR in tqdm(TRAIN_DIR):\n",
        "       for img in os.listdir(path_TRAIN_DIR):\n",
        "        if 'csv' in img:\n",
        "          continue\n",
        "        path = os.path.join(path_TRAIN_DIR, img)\n",
        "        img_data = cv2.imread(path,0) \n",
        "        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
        "        training_data.append([np.array(img_data),ord(img[6])-ord('A') ])\n",
        "\n",
        "    random.shuffle(training_data)\n",
        "\n",
        "    return training_data\n"
      ],
      "metadata": {
        "id": "AivLBvlWj5QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column1=[]\n",
        "def create_test_data():\n",
        "    test_data = []\n",
        "\n",
        "    for path_Test_DIR in tqdm(Test_DIR):\n",
        "       for img in os.listdir(path_Test_DIR):\n",
        "        if 'csv' in img:\n",
        "          continue\n",
        "        column1.append(img)\n",
        "        path = os.path.join(path_Test_DIR, img)\n",
        "        img_data = cv2.imread(path,0) \n",
        "        img_data = cv2.resize(img_data, (IMG_SIZE, IMG_SIZE))\n",
        "        test_data.append([np.array(img_data),ord(img[6])-ord('A') ])\n",
        "       \n",
        "\n",
        "    #random.shuffle(test_data)\n",
        "    return test_data\n"
      ],
      "metadata": {
        "id": "j-ujRT0IsCax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = create_train_data()\n",
        "X_train = np.array([i[0] for i in train_images]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_train = np.array([i[1] for i in train_images]).reshape(-1,)\n",
        "\n",
        "test_images = create_test_data()\n",
        "X_test = np.array([i[0] for i in test_images]).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "y_test = np.array([i[1] for i in test_images]).reshape(-1,)"
      ],
      "metadata": {
        "id": "eejyrnehsvU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model1 = Sequential([\n",
        "        layers.Conv2D(filters = 16 ,kernel_size = (3, 3), input_shape = (IMG_SIZE, IMG_SIZE, 1), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size = (2,2)),\n",
        "\n",
        "        layers.Conv2D(filters = 32 ,kernel_size = (5, 5), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    \n",
        "        layers.Conv2D(filters = 32 ,kernel_size = (5, 5), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "    \n",
        "        layers.Conv2D(filters = 16 ,kernel_size = (3, 3), padding='same', activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D(pool_size=(2,2)),\n",
        "  \n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        layers.Dense(5, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model1.compile(\n",
        "    optimizer= tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "    loss=SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy'])\n",
        "if (os.path.exists('/content/drive/MyDrive/person_Recognetion.h5')):\n",
        "      model1 = load_model('/content/drive/MyDrive/person_Recognetion.h5')\n",
        "else:\n",
        "      model1.fit(X_train, y_train, epochs = 25)\n",
        "      model1.save('/content/drive/MyDrive/person_Recognetion.h5')\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "fS-3mhEZt_mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model1.evaluate(X_test,y_test))"
      ],
      "metadata": {
        "id": "cG7HePfrstc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test=model1.predict(X_test)\n",
        "print(len(X_test))\n",
        "prediction=[]\n",
        "for i in Y_test:\n",
        "  prediction.append(np.argmax(i))\n",
        "data={'image_name':column1,'label':prediction[:40]}\n",
        "print(len(column1),len(prediction))\n",
        "df_person=pd.DataFrame(data)\n",
        "print(df_person)\n"
      ],
      "metadata": {
        "id": "ImwoJGryeZMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}